{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmOTsz/mKNNoAlLJA0Yy21",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaberFaisal/NLP_Text-classification_HW/blob/main/Natural_Language_Processing_with_Disaster_Tweet_LSTM_%26_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Architecture/Hyperparameter Choices Architecture (BiLSTM & RNN):**\n",
        "\n",
        "BiLSTM (Bidirectional LSTM):\n",
        "\n",
        "Chosen to capture bidirectional context in text (past and future word dependencies), which is critical for understanding disaster-related language nuances.\n",
        "\n",
        "LSTM’s memory cells mitigate the vanishing gradient problem, making it suitable for sequential data like text.\n",
        "\n",
        "RNN:\n",
        "\n",
        "Used as a baseline model to compare against LSTM. While simpler, RNNs struggle with long-term dependencies but are computationally lighter.\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "Embedding Size (300):\n",
        "\n",
        "Aligned with standard pre-trained embeddings (e.g., GloVe 300D), though not explicitly used here. Balances dimensionality and computational cost.\n",
        "\n",
        "Hidden Size (512):\n",
        "\n",
        "Provides sufficient capacity to learn complex patterns without excessive parameters.\n",
        "\n",
        "Layers (2):\n",
        "\n",
        "Adds depth for hierarchical feature extraction while avoiding overfitting.\n",
        "\n",
        "Dropout (0.4):\n",
        "\n",
        "Regularizes the model to prevent overfitting, especially important with limited training data.\n",
        "\n",
        "Learning Rate (0.0005):\n",
        "\n",
        "A small rate ensures stable gradient descent in AdamW optimizer.\n",
        "\n",
        "Vocabulary Size (20,000):\n",
        "\n",
        "Limits the vocabulary to frequent words, reducing noise and computational load."
      ],
      "metadata": {
        "id": "BgEiMTl1XWHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Necessary Libraries**\n",
        "\n",
        "We import essential libraries such as pandas for data handling, numpy for numerical computations, re and string for text preprocessing, nltk for NLP tasks, torch for deep learning, and sklearn for data splitting."
      ],
      "metadata": {
        "id": "IcB8B6anOb_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "ev6p5Ai6K3yG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Necessary NLTK Resources**\n",
        "\n",
        "To ensure tokenization, stopword removal, and lemmatization work properly, we download required datasets from nltk."
      ],
      "metadata": {
        "id": "HsyN_rUxOsdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjQSsGZkK7Ih",
        "outputId": "182510a7-5e4b-40f9-e7eb-8bba41702049"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**\n",
        "\n",
        "This dataset contains text data and labels (0 for non-disaster and 1 for disaster tweets)."
      ],
      "metadata": {
        "id": "krcr_TY_O-Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train (1).csv')"
      ],
      "metadata": {
        "id": "pVxTDLTjLAaV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing the text data**\n",
        "\n",
        "Cleans and preprocesses text data.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Convert text to lowercase to ensure uniformity.\n",
        "\n",
        "2. Remove URLs to clean the text.\n",
        "\n",
        "3. Remove punctuation to reduce noise.\n",
        "   \n",
        "4. Tokenize text into words.\n",
        "    \n",
        "5. Remove stopwords to focus on meaningful words.\n",
        "    \n",
        "6. Apply lemmatization to get the root form of words.\n",
        "    "
      ],
      "metadata": {
        "id": "SaZ5gd44PtJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(f'[{string.punctuation}]', '', text)  # Remove punctuation\n",
        "    words = word_tokenize(text)  # Tokenization\n",
        "    words = [word for word in words if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "id": "P9HJQ4a_NsmD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "q7SkTnXFNvt2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization and Padding**\n",
        "\n",
        "We convert text into numerical sequences using the Tokenizer class. Padding ensures all sequences have the same length for model training."
      ],
      "metadata": {
        "id": "R6_CAF9HR31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df['cleaned_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "ZCOFGc8ELKox"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Data into Training and Testing Sets**\n",
        "\n",
        "We divide the dataset into 80% training and 20% testing data for model evaluation.We convert the data into tensors so that it can be used with PyTorch for training."
      ],
      "metadata": {
        "id": "itX-yj7xR9FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['target'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "-iF-JYJrLNHc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataset and DataLoader**\n",
        "\n",
        "We create a PyTorch Dataset class to load data efficiently in batches for training and evaluation."
      ],
      "metadata": {
        "id": "x0p4S1R2S-7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "1NHOtOBBLQgG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define LSTM And RNN Model**"
      ],
      "metadata": {
        "id": "1o5kw9LMTMdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        x = self.dropout(lstm_out[:, -1, :])\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "MsPFmZmnLYQJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers, dropout):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        rnn_out, _ = self.rnn(x)\n",
        "        x = self.dropout(rnn_out[:, -1, :])\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "W0aAtvoKXxry"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Model Parameters and Initialize Model**\n",
        "\n",
        "Hyperparameters like embedding size, hidden size, and number of layers are chosen based on best practices for text classification."
      ],
      "metadata": {
        "id": "v0-EJpaLTZ-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "embed_size = 300\n",
        "hidden_size = 512\n",
        "output_size = 2\n",
        "num_layers = 2\n",
        "dropout = 0.4\n"
      ],
      "metadata": {
        "id": "cq6OB5O4LaI0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = RNNClassifier(vocab_size, embed_size, hidden_size, output_size, num_layers, dropout)\n",
        "lstm_model = BiLSTMClassifier(vocab_size, embed_size, hidden_size, output_size, num_layers, dropout)\n"
      ],
      "metadata": {
        "id": "ozeEekkbX9--"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Loss Function and Optimizer**\n",
        "\n",
        "We use CrossEntropyLoss for binary classification and Adam optimizer for efficient training."
      ],
      "metadata": {
        "id": "EbpMPtcHTjDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_rnn = optim.AdamW(rnn_model.parameters(), lr=0.0005)\n",
        "optimizer_lstm = optim.AdamW(lstm_model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "QTdeHSC6LirK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model**\n",
        "\n",
        "We train the model for 5 epochs, updating weights using backpropagation."
      ],
      "metadata": {
        "id": "rWgoevbyTvlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_checkpoint_path = \"rnn_model_checkpoint.pth\"\n",
        "lstm_checkpoint_path = \"lstm_model_checkpoint.pth\"\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, loss, checkpoint_path):\n",
        "    \"\"\"Save training checkpoint.\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    \"\"\"Load training checkpoint if available.\"\"\"\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(f\"Resuming from epoch {checkpoint['epoch']+1}\")\n",
        "        return checkpoint['epoch'] + 1\n",
        "    return 0\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=2, checkpoint_path=\"model_checkpoint.pth\"):\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "        save_checkpoint(epoch, model, optimizer, avg_loss, checkpoint_path)\n",
        "\n",
        "print(\"Training LSTM Model...\")\n",
        "train_model(lstm_model, train_loader, criterion, optimizer_lstm, epochs=2, checkpoint_path=lstm_checkpoint_path)\n",
        "print(\"Training RNN Model\")\n",
        "train_model(rnn_model, train_loader, criterion, optimizer_rnn, epochs=2, checkpoint_path=rnn_checkpoint_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "lxaY1oJrLlT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db6d946-83b3-46d6-e37b-43f5ceca593e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM Model...\n",
            "Epoch 1/2, Loss: 0.6889\n",
            "Checkpoint saved at epoch 1\n",
            "Epoch 2/2, Loss: 0.6851\n",
            "Checkpoint saved at epoch 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model**\n",
        "\n",
        "We calculate accuracy on the test dataset to measure model performance."
      ],
      "metadata": {
        "id": "B9_2MSnrT7eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {correct / total:.4f}')\n"
      ],
      "metadata": {
        "id": "VfXnZKB9Lp8k"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating RNN Model\")\n",
        "evaluate_model(rnn_model, test_loader)"
      ],
      "metadata": {
        "id": "BqtT1UYBZAZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a063c2-f775-49be-88eb-913179fdfa9b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RNN Model\n",
            "Accuracy: 0.5739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating LSTM Model\")\n",
        "evaluate_model(lstm_model, test_loader)"
      ],
      "metadata": {
        "id": "ltFUeYf_aIzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f78a29-e4e1-4273-b868-e2395151338e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating LSTM Model\n",
            "Accuracy: 0.5739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Predictions for Kaggle Submission**"
      ],
      "metadata": {
        "id": "kOJMxorQNJUy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "O8vNy0gfIR0s"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('/content/test (1).csv')\n",
        "test_df['cleaned_text'] = test_df['text'].apply(preprocess_text)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df['cleaned_text'])\n",
        "test_padded = pad_sequences(test_sequences, maxlen=100, padding='post', truncating='post')\n",
        "test_tensor = torch.tensor(test_padded, dtype=torch.long)\n",
        "\n",
        "model = rnn_model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_tensor)\n",
        "    _, test_predictions = torch.max(test_outputs, 1)\n",
        "\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'target': test_predictions.numpy()})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "Best Model in Terms of Quality/Resources:\n",
        "\n",
        "Both RNN and BiLSTM achieved identical test accuracy (57.39%), suggesting underfitting or insufficient training (only 2 epochs).\n",
        "\n",
        "BiLSTM theoretically offers better quality due to bidirectional context but requires more resources (memory and computation).RNN is lighter but less effective for sequential dependencies.\n",
        "\n",
        "How to Improve Results:\n",
        "\n",
        "Increase Training Epochs: Train for more epochs (e.g., 10–20) to allow convergence.\n",
        "\n",
        "Hyperparameter Tuning: Experiment with embedding/hidden sizes, dropout rates, and learning rates.\n",
        "\n",
        "Pre-trained Embeddings: Use GloVe or BERT embeddings for better word representations.\n",
        "\n",
        "Attention Mechanisms: Add attention layers to focus on critical words.\n",
        "\n",
        "Data Augmentation: Expand the dataset with synonym replacement or back-translation.\n",
        "\n",
        "Class Balancing: Address potential class imbalance in disaster/non-disaster tweets.\n",
        "\n",
        "Difficulties Encountered:\n",
        "\n",
        "Low Accuracy: The models underperformed (57.39% accuracy), likely due to:\n",
        "Insufficient training time (2 epochs).\n",
        "\n",
        "Over-aggressive preprocessing (e.g., removing punctuation might discard context like “!” in disaster tweets).\n",
        "\n",
        "Limited model capacity or suboptimal hyperparameters.\n",
        "\n",
        "Framework Mixing: Using Keras (Tokenizer) with PyTorch for modeling could introduce inconsistencies.\n",
        "\n",
        "Recommendation\n",
        "The BiLSTM holds more potential with adjustments (more epochs, tuned hyperparameters, and pre-trained embeddings). For immediate improvements, prioritize extended training and embedding enhancements."
      ],
      "metadata": {
        "id": "8G7igWK0YGlJ"
      }
    }
  ]
}